---
title: Vercel AI SDK Example
description: LLM streaming with Vercel AI SDK
---

import AiSdkExample from './AiSdkExample';
import GithubFiles from '@/components/GithubFiles';
import Example from '@/components/Example';

# Vercel AI SDK Example

The `result.toUIMessageStreamResponse` method returns a `Response` object, which is forwarded directly to the Next.js route handler. On the client side, you can use the `useChat` hook to consume the stream and render chat messages seamlessly, as it's outlined in the [AI SDK](https://github.com/vercel/ai) documentation.

## Result

<Example>
  <AiSdkExample />
</Example>

## Code

<GithubFiles paths={['src/modules/ai-sdk/AiSdkController.ts', 'src/app/ai-sdk/AiSdkExample.tsx']} />

## Related Documentation

- [LLM Completions](https://vovk.dev/llm)
- [Function Calling](https://vovk.dev/function-calling)
- [TypeScript RPC](https://vovk.dev/typescript)
- [Controller](https://vovk.dev/controller)
- [`@operation` Decorator](https://vovk.dev/openapi)